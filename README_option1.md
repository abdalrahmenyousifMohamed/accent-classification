# ğŸ™ï¸ Whisper ASR + Pyannote for Accent Categorization

## ğŸ§© Introduction

Accent categorization is the task of identifying a speakerâ€™s accent from spoken audio (e.g., Indian English, American English, British English). It plays a vital role in:

- Speech analytics  
- Language learning technology  
- Tuning speech-to-text models  
- Quality assurance for call centers  
- Regional content localization  

This project combines **speaker diarization** and **automatic speech recognition (ASR)** to segment, transcribe, and analyze speech per speaker to determine their accent.

---

## ğŸ”§ Technologies Used

| Component               | Purpose                            | Tool                                |
|------------------------|------------------------------------|-------------------------------------|
| ASR                    | Transcribe spoken words            | OpenAI Whisper                      |
| Speaker Diarization    | Identify and segment speakers      | pyannote-audio                      |
| Timestamp Alignment    | Align segments with transcript     | Custom timestamp merging logic      |
| Feature Extraction     | Extract embeddings for analysis    | e.g., x-vectors, Wav2Vec2, MFCC     |
| Accent Classifier      | (Optional) Classify accents        | Custom model (XGBoost, Transformer) |

---

## ğŸ§  Why This Setup Works for Accent Categorization

### 1. ğŸ¯ Speaker-Aware Accent Detection  
- **Challenge**: Accent is a speaker-level trait, not per utterance.  
- **Solution**: Use diarization to group all audio per speaker.  
- **Benefit**: More accurate and personalized accent categorization.

### 2. âœ¨ Whisperâ€™s Robust Multilingual Transcription  
- Trained on diverse, multi-accent corpora  
- Maintains performance across heavily-accented English  
- Transcripts can support phoneme-level or linguistic feature analysis

### 3. ğŸ” Pyannoteâ€™s Effective Speaker Segmentation  
- Strong performance in real-world noisy scenarios  
- Supports overlapping speech  
- Provides speaker embeddings to group identity across segments

### 4. ğŸ§¬ ECAPA-TDNN for Speaker Embedding Extraction
- Utilizes the speechbrain/spkrec-ecapa-voxceleb model
- Extracts high-quality speaker embeddings using attentive statistical pooling
- Embeddings can be used for clustering and accent classification

> This pipeline becomes a **scalable preprocessing system** for accent analysis.

---

## ğŸ” Workflow Summary

```plaintext
Input Audio
   â†“
[pyannote-audio]
â†’ Diarize: (speaker, start, end)
   â†“
[Whisper]
â†’ Transcribe + Timestamps
   â†“
Merge Transcripts with Speaker Segments
   â†“
Per Speaker:
    â†’ Extract audio from all turns
    â†’ (Optional) Normalize loudness, remove silences
    â†’ Extract acoustic embeddings
    â†’ Run accent classifier or clustering
   â†“
Output: (accent label)
```

---

## ğŸ› ï¸ Implementation Notes

- **Speaker Embedding Extraction**:  the speechbrain/spkrec-ecapa-voxceleb model to extract 192-dimensional speaker embeddings.
- **Clustering Speaker Embeddings**: Helps correct diarization errors and improves speaker grouping.
- **Language Filtering**: Whisperâ€™s language ID can validate that the audio is English before accent classification.
- **Accent Training Data**:  labeled datasets like Common Voice (with accent tags) or L2-Arctic for training classifiers.

---

## âš ï¸ Limitations

- **Diarization Errors**: False splits/merges can skew accent predictions.  
- **ASR Bias**: Whisper may transcribe differently depending on accent.  
- **Training Data**: Accent classifiers need well-curated, labeled datasets.  
- **Granularity**: Accent is a spectrumâ€”broad labels (e.g., "British", "American") may oversimplify real-world nuances.

